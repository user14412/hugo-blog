+++
date = '2026-01-29T22:01:55+08:00'
draft = false
title = 'äº¤å‰ç†µæŸå¤±å‡½æ•°ç†è®ºä¸å®è·µ'
description = ''
tags = ['æ·±åº¦å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ', 'æŸå¤±å‡½æ•°']
categories = []
author = ''
math = true
+++

CrossEntropyLoss æ˜¯æ·±åº¦å­¦ä¹ ä¸­**æœ€å¸¸ç”¨çš„åˆ†ç±»é—®é¢˜æŸå¤±å‡½æ•°**ï¼Œç‰¹åˆ«é€‚åˆå¤šåˆ†ç±»ä»»åŠ¡ã€‚

### ğŸ”‘ å…³é”®ç‚¹ï¼š

- **è¾“å…¥**ï¼šæ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼ˆlogitsï¼‰ï¼Œä¸éœ€è¦ç»è¿‡ softmax
- **ç›®æ ‡**ï¼šçœŸå®çš„ç±»åˆ«æ ‡ç­¾
- **ä½œç”¨**ï¼šè¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚

æˆ‘ä¼šä»ç›´è§‚ç†è§£åˆ°æ•°å­¦åŸç†ï¼Œé€æ­¥è®²è§£äº¤å‰ç†µçš„æ ¸å¿ƒæ¦‚å¿µã€‚

---

## 1ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦æŸå¤±å‡½æ•°ï¼Ÿ

è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯ï¼š**è®©æ¨¡å‹çš„é¢„æµ‹è¶Šæ¥è¶Šæ¥è¿‘çœŸå®ç­”æ¡ˆ**ã€‚

- **æŸå¤±å‡½æ•°**ï¼šè¡¡é‡é¢„æµ‹ä¸çœŸå®å€¼çš„å·®è·
- æŸå¤±è¶Šå° â†’ é¢„æµ‹è¶Šå‡†ç¡®
- è®­ç»ƒç›®æ ‡ï¼šæœ€å°åŒ–æŸå¤±å‡½æ•°

---

## 2ï¸âƒ£ ä¿¡æ¯è®ºåŸºç¡€ï¼šç†µï¼ˆEntropyï¼‰

### ğŸ“Œ ä»€ä¹ˆæ˜¯"ä¿¡æ¯é‡"ï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šè¶Šä¸å¯èƒ½å‘ç”Ÿçš„äº‹ä»¶ï¼Œä¿¡æ¯é‡è¶Šå¤§ã€‚

```
ä¾‹å­ï¼š
- "å¤ªé˜³ä»ä¸œè¾¹å‡èµ·" â†’ ä¿¡æ¯é‡å°ï¼ˆåºŸè¯ï¼Œæ¯å¤©éƒ½è¿™æ ·ï¼‰
- "æ˜å¤©ä¸‹é‡‘å­é›¨" â†’ ä¿¡æ¯é‡å¤§ï¼ˆå¤ªç½•è§äº†ï¼ï¼‰
```

**æ•°å­¦å®šä¹‰**ï¼šäº‹ä»¶æ¦‚ç‡ä¸º $p$ï¼Œä¿¡æ¯é‡ä¸ºï¼š

$$
I(x) = -\log(p)
$$

```python
import numpy as np

# æ¦‚ç‡ 0.99 çš„äº‹ä»¶ï¼ˆå‡ ä¹ç¡®å®šå‘ç”Ÿï¼‰
info_certain = -np.log(0.99)
print(f"é«˜æ¦‚ç‡äº‹ä»¶ä¿¡æ¯é‡: {info_certain:.4f}")

# æ¦‚ç‡ 0.01 çš„äº‹ä»¶ï¼ˆå‡ ä¹ä¸å¯èƒ½ï¼‰
info_rare = -np.log(0.01)
print(f"ä½æ¦‚ç‡äº‹ä»¶ä¿¡æ¯é‡: {info_rare:.4f}")
```

**è¾“å‡º**ï¼š

```
é«˜æ¦‚ç‡äº‹ä»¶ä¿¡æ¯é‡: 0.0101
ä½æ¦‚ç‡äº‹ä»¶ä¿¡æ¯é‡: 4.6052
```

### ğŸ“Œ ä»€ä¹ˆæ˜¯"ç†µ"ï¼Ÿ

**ç†µ = ä¿¡æ¯é‡çš„æœŸæœ›å€¼ = ä¸ç¡®å®šæ€§çš„åº¦é‡**

$$
H(P) = -\sum_{i=1}^{n} p_i \log(p_i)
$$

```python
def entropy(probs):
    return -np.sum(probs * np.log(probs + 1e-10))

# ç¡®å®šæ€§åˆ†å¸ƒï¼ˆå®Œå…¨ç¡®å®šï¼‰
certain = np.array([1.0, 0.0, 0.0])
print(f"ç¡®å®šåˆ†å¸ƒçš„ç†µ: {entropy(certain):.4f}")

# å‡åŒ€åˆ†å¸ƒï¼ˆæœ€ä¸ç¡®å®šï¼‰
uniform = np.array([1/3, 1/3, 1/3])
print(f"å‡åŒ€åˆ†å¸ƒçš„ç†µ: {entropy(uniform):.4f}")

# ä¸­ç­‰ä¸ç¡®å®šæ€§
medium = np.array([0.7, 0.2, 0.1])
print(f"ä¸­ç­‰åˆ†å¸ƒçš„ç†µ: {entropy(medium):.4f}")
```

**è¾“å‡º**ï¼š

```
ç¡®å®šåˆ†å¸ƒçš„ç†µ: 0.0000      â† æ— ä¸ç¡®å®šæ€§
å‡åŒ€åˆ†å¸ƒçš„ç†µ: 1.0986      â† æœ€å¤§ä¸ç¡®å®šæ€§
ä¸­ç­‰åˆ†å¸ƒçš„ç†µ: 0.8018      â† ä¸­ç­‰ä¸ç¡®å®šæ€§
```

**ç›´è§‚ç†è§£**ï¼š

- ç†µè¶Šå¤§ â†’ è¶Šæ··ä¹±ã€è¶Šä¸ç¡®å®š
- ç†µè¶Šå° â†’ è¶Šæœ‰åºã€è¶Šç¡®å®š

---

## 3ï¸âƒ£ äº¤å‰ç†µï¼ˆCross Entropyï¼‰

### ğŸ“Œ å®šä¹‰

**äº¤å‰ç†µ**ï¼šç”¨åˆ†å¸ƒ $Q$ æ¥ç¼–ç åˆ†å¸ƒ $P$ æ‰€éœ€çš„å¹³å‡ä¿¡æ¯é‡ã€‚

$$
H(P, Q) = -\sum_{i=1}^{n} p_i \log(q_i)
$$

å…¶ä¸­ï¼š

- $P$ï¼šçœŸå®åˆ†å¸ƒï¼ˆground truthï¼‰
- $Q$ï¼šé¢„æµ‹åˆ†å¸ƒï¼ˆmodel predictionï¼‰

### ğŸ“Œ ç›´è§‚ä¾‹å­ï¼šå¤©æ°”é¢„æŠ¥

```
çœŸå®æƒ…å†µï¼šæ˜å¤© 100% ä¸‹é›¨
é¢„æŠ¥å‘˜Aï¼šä¸‹é›¨ 90%ï¼Œæ™´å¤© 10%
é¢„æŠ¥å‘˜Bï¼šä¸‹é›¨ 60%ï¼Œæ™´å¤© 40%
```

```python
# çœŸå®åˆ†å¸ƒï¼ˆone-hotï¼‰
true_dist = np.array([1.0, 0.0])  # [é›¨, æ™´]

# é¢„æŠ¥å‘˜Açš„é¢„æµ‹
pred_A = np.array([0.9, 0.1])
ce_A = -np.sum(true_dist * np.log(pred_A))
print(f"é¢„æŠ¥å‘˜Açš„äº¤å‰ç†µ: {ce_A:.4f}")

# é¢„æŠ¥å‘˜Bçš„é¢„æµ‹
pred_B = np.array([0.6, 0.4])
ce_B = -np.sum(true_dist * np.log(pred_B))
print(f"é¢„æŠ¥å‘˜Bçš„äº¤å‰ç†µ: {ce_B:.4f}")
```

**è¾“å‡º**ï¼š

```
é¢„æŠ¥å‘˜Açš„äº¤å‰ç†µ: 0.1054
é¢„æŠ¥å‘˜Bçš„äº¤å‰ç†µ: 0.5108
```

**ç»“è®º**ï¼šé¢„æŠ¥å‘˜Aæ›´å‡†ç¡®ï¼ˆäº¤å‰ç†µæ›´å°ï¼‰ã€‚

---

## 4ï¸âƒ£ ä¸ºä»€ä¹ˆåˆ†ç±»é—®é¢˜ç”¨äº¤å‰ç†µï¼Ÿ

### å¯¹æ¯”å¸¸è§æŸå¤±å‡½æ•°

```python
import torch
import torch.nn as nn

# çœŸå®æ ‡ç­¾ï¼šç±»åˆ«0
target = torch.tensor([0])

# æƒ…å†µ1ï¼šé¢„æµ‹æ­£ç¡®
logits_correct = torch.tensor([[5.0, 1.0, 0.5]])

# æƒ…å†µ2ï¼šé¢„æµ‹é”™è¯¯ä½†æ¥è¿‘
logits_close = torch.tensor([[2.0, 1.8, 0.5]])

# æƒ…å†µ3ï¼šé¢„æµ‹å®Œå…¨é”™è¯¯
logits_wrong = torch.tensor([[0.5, 1.0, 5.0]])

ce_loss = nn.CrossEntropyLoss()
mse_loss = nn.MSELoss()

# å°†targetè½¬ä¸ºone-hotç”¨äºMSE
target_onehot = torch.tensor([[1.0, 0.0, 0.0]])

print("=== äº¤å‰ç†µæŸå¤± ===")
print(f"é¢„æµ‹æ­£ç¡®: {ce_loss(logits_correct, target):.4f}")
print(f"é¢„æµ‹æ¥è¿‘: {ce_loss(logits_close, target):.4f}")
print(f"é¢„æµ‹é”™è¯¯: {ce_loss(logits_wrong, target):.4f}")

print("\n=== MSEæŸå¤±ï¼ˆå¯¹æ¯”ï¼‰ ===")
print(f"é¢„æµ‹æ­£ç¡®: {mse_loss(logits_correct, target_onehot):.4f}")
print(f"é¢„æµ‹æ¥è¿‘: {mse_loss(logits_close, target_onehot):.4f}")
print(f"é¢„æµ‹é”™è¯¯: {mse_loss(logits_wrong, target_onehot):.4f}")
```

**è¾“å‡º**ï¼š

```
=== äº¤å‰ç†µæŸå¤± ===
é¢„æµ‹æ­£ç¡®: 0.0181
é¢„æµ‹æ¥è¿‘: 0.5619
é¢„æµ‹é”™è¯¯: 4.5181

=== MSEæŸå¤±ï¼ˆå¯¹æ¯”ï¼‰ ===
é¢„æµ‹æ­£ç¡®: 4.4167
é¢„æµ‹æ¥è¿‘: 2.3467
é¢„æµ‹é”™è¯¯: 6.7567
```

### ğŸ“Š ä¸ºä»€ä¹ˆäº¤å‰ç†µæ›´å¥½ï¼Ÿ

| ç‰¹æ€§           | äº¤å‰ç†µ               | MSE          |
| -------------- | -------------------- | ------------ |
| **æ¢¯åº¦ç‰¹æ€§**   | é”™è¯¯è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå¤§   | æ¢¯åº¦å¯èƒ½é¥±å’Œ |
| **æ¦‚ç‡è§£é‡Š**   | æœ‰æ˜ç¡®çš„æ¦‚ç‡æ„ä¹‰     | æ— æ¦‚ç‡æ„ä¹‰   |
| **åˆ†ç±»ä»»åŠ¡**   | âœ… ä¸“ä¸ºåˆ†ç±»è®¾è®¡       | âŒ æ›´é€‚åˆå›å½’ |
| **æ•°å€¼ç¨³å®šæ€§** | âœ… é…åˆLogSoftmaxç¨³å®š | âš ï¸ å¯èƒ½ä¸ç¨³å®š |

---

## 5ï¸âƒ£ PyTorchä¸­çš„CrossEntropyLossåšäº†ä»€ä¹ˆï¼Ÿ

### ğŸ“Œ å®Œæ•´æµç¨‹

**CrossEntropyLoss = Softmax + è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

logits = torch.tensor([[2.0, 1.0, 0.1]])
target = torch.tensor([0])

# æ–¹æ³•1ï¼šç›´æ¥ç”¨CrossEntropyLossï¼ˆæ¨èï¼‰
ce_loss = nn.CrossEntropyLoss()
loss1 = ce_loss(logits, target)

# æ–¹æ³•2ï¼šæ‰‹åŠ¨å®ç°ï¼ˆç­‰ä»·ï¼‰
log_softmax = F.log_softmax(logits, dim=1)
nll_loss = nn.NLLLoss()
loss2 = nll_loss(log_softmax, target)

# æ–¹æ³•3ï¼šå®Œå…¨æ‰‹åŠ¨
softmax = F.softmax(logits, dim=1)
loss3 = -torch.log(softmax[0, target[0]])

print(f"CrossEntropyLoss: {loss1:.4f}")
print(f"LogSoftmax + NLL: {loss2:.4f}")
print(f"æ‰‹åŠ¨è®¡ç®—:        {loss3:.4f}")
```

**è¾“å‡º**ï¼š

```
CrossEntropyLoss: 0.4170
LogSoftmax + NLL: 0.4170
æ‰‹åŠ¨è®¡ç®—:        0.4170
```

### ğŸ“Œ æ•°å­¦æ¨å¯¼

ç»™å®š logits $z_1, z_2, ..., z_C$ å’ŒçœŸå®ç±»åˆ« $y$ï¼š

**æ­¥éª¤1ï¼šSoftmax**
$$
p_i = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
$$

**æ­¥éª¤2ï¼šå–è´Ÿå¯¹æ•°**
$$
L = -\log(p_y) = -\log\left(\frac{e^{z_y}}{\sum_{j=1}^{C} e^{z_j}}\right)
$$

**æ­¥éª¤3ï¼šåŒ–ç®€ï¼ˆLogSumExpæŠ€å·§ï¼‰**
$$
L = -z_y + \log\left(\sum_{j=1}^{C} e^{z_j}\right)
$$

```python
# æ‰‹åŠ¨å®ç°å®Œæ•´è®¡ç®—
def manual_cross_entropy(logits, target):
    z_y = logits[0, target]
    log_sum_exp = torch.log(torch.sum(torch.exp(logits)))
    return -z_y + log_sum_exp

loss_manual = manual_cross_entropy(logits, target)
print(f"å®Œå…¨æ‰‹åŠ¨å®ç°: {loss_manual:.4f}")
```

---

## 6ï¸âƒ£ æ¢¯åº¦åˆ†æï¼šä¸ºä»€ä¹ˆæ”¶æ•›å¿«ï¼Ÿ

### ğŸ“Œ äº¤å‰ç†µçš„æ¢¯åº¦

å¯¹äº logits $z_i$ï¼ŒæŸå¤±å¯¹ $z_i$ çš„æ¢¯åº¦ä¸ºï¼š

$$
\frac{\partial L}{\partial z_i} = p_i - y_i
$$

å…¶ä¸­ï¼š

- $p_i$ï¼šSoftmaxåçš„é¢„æµ‹æ¦‚ç‡
- $y_i$ï¼šçœŸå®æ ‡ç­¾ï¼ˆone-hotï¼‰

**å…³é”®ç‰¹æ€§**ï¼šæ¢¯åº¦ = é¢„æµ‹å€¼ - çœŸå®å€¼

```python
# æ¢¯åº¦å¯è§†åŒ–
logits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)
target = torch.tensor([0])

loss = ce_loss(logits, target)
loss.backward()

print("Logits:", logits.data.numpy())
print("Softmax:", F.softmax(logits, dim=1).data.numpy())
print("æ¢¯åº¦:", logits.grad.numpy())
print("çœŸå®æ ‡ç­¾(one-hot):", np.array([[1.0, 0.0, 0.0]]))
```

**è¾“å‡º**ï¼š

```
Logits: [[2.  1.  0.1]]
Softmax: [[0.659  0.242  0.099]]
æ¢¯åº¦: [[-0.341  0.242  0.099]]
çœŸå®æ ‡ç­¾(one-hot): [[1. 0. 0.]]
```

**è§£é‡Š**ï¼š

- ç±»åˆ«0ï¼ˆæ­£ç¡®ï¼‰ï¼šæ¢¯åº¦ = 0.659 - 1 = -0.341ï¼ˆå‘ä¸Šè°ƒæ•´ï¼‰
- ç±»åˆ«1ï¼ˆé”™è¯¯ï¼‰ï¼šæ¢¯åº¦ = 0.242 - 0 = 0.242ï¼ˆå‘ä¸‹è°ƒæ•´ï¼‰
- ç±»åˆ«2ï¼ˆé”™è¯¯ï¼‰ï¼šæ¢¯åº¦ = 0.099 - 0 = 0.099ï¼ˆå‘ä¸‹è°ƒæ•´ï¼‰

### ğŸ“Œ å¯¹æ¯”MSEçš„æ¢¯åº¦é—®é¢˜

```python
# MSEåœ¨Sigmoidé¥±å’ŒåŒºæ¢¯åº¦æ¶ˆå¤±çš„ä¾‹å­
x = torch.tensor([10.0], requires_grad=True)  # é¥±å’ŒåŒº
target = torch.tensor([1.0])

sigmoid = torch.sigmoid(x)
mse = (sigmoid - target) ** 2

mse.backward()
print(f"Sigmoidè¾“å‡º: {sigmoid.item():.6f}")
print(f"MSEæ¢¯åº¦: {x.grad.item():.10f}")  # æ¥è¿‘0ï¼
```

**è¾“å‡º**ï¼š

```
Sigmoidè¾“å‡º: 0.999955
MSEæ¢¯åº¦: 0.0000408851
```

**é—®é¢˜**ï¼šå³ä½¿é¢„æµ‹é”™è¯¯ï¼Œæ¢¯åº¦ä¹Ÿå‡ ä¹ä¸º0ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚

---

## 7ï¸âƒ£ å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆé‡è¦æ‰©å±•ï¼‰

æœ‰æ—¶ä¸€ä¸ªæ ·æœ¬å¯ä»¥å±äºå¤šä¸ªç±»åˆ«ï¼ˆå¦‚å›¾ç‰‡åŒæ—¶åŒ…å«çŒ«å’Œç‹—ï¼‰ã€‚

### ä½¿ç”¨ Binary Cross Entropy

```python
# å¤šæ ‡ç­¾ï¼š[çŒ«, ç‹—, é¸Ÿ]
bce_loss = nn.BCEWithLogitsLoss()

# æ ·æœ¬1ï¼šåŒ…å«çŒ«å’Œç‹—
logits = torch.tensor([[2.0, 1.5, -1.0]])
target = torch.tensor([[1.0, 1.0, 0.0]])

loss = bce_loss(logits, target)
print(f"å¤šæ ‡ç­¾æŸå¤±: {loss:.4f}")

# æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªç±»åˆ«çš„BCE
def binary_ce(logit, label):
    sigmoid = torch.sigmoid(logit)
    return -(label * torch.log(sigmoid) + (1-label) * torch.log(1-sigmoid))

manual = (binary_ce(logits[0,0], target[0,0]) + 
          binary_ce(logits[0,1], target[0,1]) + 
          binary_ce(logits[0,2], target[0,2])) / 3
print(f"æ‰‹åŠ¨è®¡ç®—: {manual:.4f}")
```

---

## 8ï¸âƒ£ å®æˆ˜æŠ€å·§

### âœ… æŠ€å·§1ï¼šåˆ«å¯¹logitsåšsoftmax

```python
# âŒ é”™è¯¯
logits = model(x)
probs = F.softmax(logits, dim=1)
loss = ce_loss(probs, target)  # æ•°å€¼ä¸ç¨³å®šï¼

# âœ… æ­£ç¡®
logits = model(x)
loss = ce_loss(logits, target)  # å†…éƒ¨æœ‰ä¼˜åŒ–
```

### âœ… æŠ€å·§2ï¼šç±»åˆ«ä¸å¹³è¡¡ç”¨weight

```python
# æ•°æ®é›†ï¼šçŒ«1000ï¼Œç‹—100ï¼Œé¸Ÿ10
# åæ¯”ä¾‹æƒé‡
weight = torch.tensor([1.0, 10.0, 100.0])
criterion = nn.CrossEntropyLoss(weight=weight)
```

### âœ… æŠ€å·§3ï¼šæ ‡ç­¾å¹³æ»‘é˜²æ­¢è¿‡æ‹Ÿåˆ

```python
# å°†ç¡¬æ ‡ç­¾ [1, 0, 0] è½¯åŒ–ä¸º [0.9, 0.05, 0.05]
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

**åŸç†**ï¼š

- ç¡¬æ ‡ç­¾ï¼šæ¨¡å‹å­¦ä¹ è¾“å‡ºæ— é™å¤§çš„logitsï¼ˆè¿‡æ‹Ÿåˆï¼‰
- è½¯æ ‡ç­¾ï¼šé¼“åŠ±æ¨¡å‹è¾“å‡ºæœ‰é™çš„ç½®ä¿¡åº¦

### âœ… æŠ€å·§4ï¼šFocal Lossï¼ˆéš¾æ ·æœ¬æŒ–æ˜ï¼‰

å¤„ç†æåº¦ä¸å¹³è¡¡çš„æ•°æ®ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ï¼‰ï¼š

$$
FL = -(1-p_t)^\gamma \log(p_t)
$$

```python
class FocalLoss(nn.Module):
    def __init__(self, gamma=2):
        super().__init__()
        self.gamma = gamma
        self.ce = nn.CrossEntropyLoss(reduction='none')
    
    def forward(self, logits, target):
        ce_loss = self.ce(logits, target)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        return focal_loss.mean()

# æµ‹è¯•
focal = FocalLoss(gamma=2)
logits = torch.tensor([[5.0, 0.0, 0.0]])  # ç®€å•æ ·æœ¬ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
target = torch.tensor([0])

print(f"æ™®é€šCE: {ce_loss(logits, target):.4f}")
print(f"Focal Loss: {focal(logits, target):.4f}")  # æ›´å°ï¼ˆé™ä½ç®€å•æ ·æœ¬æƒé‡ï¼‰
```

---

## 9ï¸âƒ£ æ€»ç»“å¯¹æ¯”è¡¨

| æ¦‚å¿µ           | å…¬å¼                      | å«ä¹‰     | åº”ç”¨            |
| -------------- | ------------------------- | -------- | --------------- |
| **ç†µ**         | $-\sum p \log p$          | ä¸ç¡®å®šæ€§ | è¡¡é‡åˆ†å¸ƒæ··ä¹±åº¦  |
| **äº¤å‰ç†µ**     | $-\sum p \log q$          | ç¼–ç ä»£ä»· | åˆ†ç±»æŸå¤±å‡½æ•°    |
| **KLæ•£åº¦**     | $\sum p \log \frac{p}{q}$ | åˆ†å¸ƒè·ç¦» | ç”Ÿæˆæ¨¡å‹ï¼ˆVAEï¼‰ |
| **Focal Loss** | $-(1-p)^\gamma \log p$    | éš¾ä¾‹æŒ–æ˜ | ç›®æ ‡æ£€æµ‹        |

---

## ğŸ¯ è®°å¿†è¦ç‚¹

1. **äº¤å‰ç†µ = ç”¨é”™è¯¯åˆ†å¸ƒç¼–ç æ­£ç¡®åˆ†å¸ƒçš„ä»£ä»·**
2. **è¶Šè‡ªä¿¡åœ°é”™ï¼Œæƒ©ç½šè¶Šå¤§**ï¼ˆæŒ‡æ•°çº§å¢é•¿ï¼‰
3. **æ¢¯åº¦ = é¢„æµ‹ - çœŸå®**ï¼ˆå¤©ç„¶é€‚åˆä¼˜åŒ–ï¼‰
4. **å†…éƒ¨åŒ…å«Softmaxï¼Œè¾“å…¥ç›´æ¥ç”¨logits**
5. **å¤šåˆ†ç±»ç”¨CEï¼Œå¤šæ ‡ç­¾ç”¨BCE**

æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿ç»§ç»­æ·±å…¥è®¨è®ºï¼ğŸš€